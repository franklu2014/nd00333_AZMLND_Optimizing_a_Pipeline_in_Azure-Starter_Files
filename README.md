# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This project analyzes and models a dataset containing customer information about bank marketing, and we aim to predicting the outcome of a marketing strerategy based on the customer's education, marital status, etc.

We attempted this experiment with a HyperDrive run and an AutoML run. The best performing model was a VotingEnsemble classifier produced by AutoML that has a accuacy of 0.916 and a weighted ROC AUC of 0.950.  As a comparison, the LogisticRegression model has an accuracy of 0.911 and a weighted ROC AUC of 0.930.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

We use a SKLearn estimator combined with HyperDrive for hyperparameter tuning.

The SKLearn estimator excutes the "train.py" script, which downloads the dataset, transforms categorical features with One-Hot encoding, splits data into train set and test set, fits a LogisticRegression classifier against hyperparameters passed from HyperDrive, and then evaluates and records the model's 'ROC-AUC' score.

The HyperDrive is configured to sample the `C` regularization strength hyperparameters from a user-defined distribution and pass this hyperparameter to the SKLearn estimator.  The performance of each experiment run is recorded, and an early stop policy is in place to terminate under-performing runs earlier.

After the count of max_total_runs is reached, we can look into the best performing run and its hyperparameter.

**What are the benefits of the parameter sampler you chose?**

In the configuration of HyperDrive, we use RandomParameterSampling to randomly sample the `C` papameter from a Uniform distribution between [0.01, 10].  HyperDrive has 3 samplers: GridParameterSampling, BayesianParameterSampling, and RandomParameterSampling.  GridParameterSampling works with discrete parameters and isn't suitable for our case here.  BayesianParameterSampling requires much more resources to explore the hyperparameter space, which might exceed the limits on experiment VM.  On the other hand, RandomParameterSampling works with continuous parameters and requires less resource, therefore it's chosen.

**What are the benefits of the early stopping policy you chose?**

We use BanditPolicy for early stopping.  The benefit of BanditPolicy is that it will terminate a run quickly if its performance of earlier iterations is less than a certain percentage from the best run recorded.  Because we have very limited VM time, the aggresive cancellation of BanditPolicy is what we need.  If we were provided with more VM time, MedianStoppingPolicy can be a good choice because it doesn't terminate runs as agressively as BanditPolicy, and we might see more promising training jobs executed til the end.  TruncationSelectionPolicy is more tolerant of low-performing runs, which will take even more time to finish and is not a good choice in our situation. 

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

In the AutoML configuration, we set the task type to 'classification' and the primary metric to 'AUC_weighted'. The best model produced by AutoML is a VotingEnsemble that consists of underlying MaxAbsScaler for data scaling and  LightGBMClassifier for classification.

From the notebook output, some of the hyperparameters of the LightGBMClassifier are:
- boosting_type='gbdt'
- class_weight=None
- colsample_bytree=1.0
- importance_type='split'
- learning_rate=0.1
- max_depth=-1
- min_child_samples=20
- min_child_weight=0.001

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The LogisticRegression model tuned with HyperDrive has 0.909 for accuracy and 0.928 for weighted ROC AUC.  The VotingEnsemble produced by AutoML has 0.917 for accuracy and 0.950 for weighted ROC AUC.  The VotingEnsemble model has higher accuracy, and its higher ROC AUC score also indicate a lower chance of false positive cases.

Architecture-wise, the VotingEnsemble classifier collects predictions from multiple underlying algorithms and shows the most likely result.  On the other hand, LogisticRegression uses a linear function that predict the outcomes based on passed-in features.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

If the time and resource permit, I would like to try deep learning algorithms with both HyperDrive and AutoML.  As deep learning algorithms are known to discover the non-linear relations between features and lables, it will be intersting to see if the non-linear relations discovered by deep learning models can lead to a higher accuracy than traditional models.

Another improvement is about hyperparameter tunning.  From some online research, it seems that hyperparameter sampling can have 2 stages: the first stage uses RandomParameterSampling for initial hyperparameter search, and then the second stage uses BayesianParameterSampling to refine the search for the best hyperparameter.  Because the 'no' class appears more frequently than the 'yes' class, we may also include the `class_weight` hyperparameter in HyperDriveConfig.  With the additional hyperparameter and refined 2-stage tuning, we might be able to find the best set of hyperparameters and have a LogisticRegression model performing perfectly for this dataset.

## Proof of cluster clean up

As shown in the Jupyter Notebook, the cluster was deleted in the end. 
