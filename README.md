# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This project analyzes and models a dataset containing customer information about bank marketing, and we aim to predicting the outcome of a marketing strerategy based on the customer's education, marital status, etc.

We attempted this experiment with a HyperDrive run and an AutoML run. The best performing model was a VotingEnsemble classifier produced by AutoML that has a accuacy of 0.916 and a weighted ROC AUC of 0.950.  As a comparison, the LogisticRegression model has an accuracy of 0.911 and a weighted ROC AUC of 0.930.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The categorical features in the dataset are transformed with One-Hot encoding, and the dataset is split into train set and test set.  We then fit a LogisticRegression classifier with the train set and use HyperDrive to tune the `C` hyperparameter for regularization strength.

**What are the benefits of the parameter sampler you chose?**

In the configuration of HyperDrive, we use RandomParameterSampling to randomly sample the `C` papameter from a Uniform distribution between [0.01, 10].  Sampling this way can explore the parameter space randomly when the ballpark is unknown, so it will find the suitable hyperparameter faster than exhausting the possible range. 

**What are the benefits of the early stopping policy you chose?**

We use BanditPolicy for the early stopping policy.  It will terminate a run earlier if its performance doesn't meet the desired standard, so the experiment can move on to the next run more quickly.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

The best model produced by AutoML is a VotingEnsemble that consists of LightGBMClassifier instances.  For hyperparameters of this ensemble model, `n_estimators` = 600, and `num_leaves` = 104.  

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The LogisticRegression model tuned with HyperDrive has 0.909 for accuracy and 0.928 for weighted ROC AUC.  The VotingEnsemble produced by AutoML has 0.917 for accuracy and 0.950 for weighted ROC AUC.  The VotingEnsemble model has higher accuracy, and its higher ROC AUC score also indicate a lower chance of false positive cases.

Architecture-wise, the VotingEnsemble classifier collects predictions from multiple underlying algorithms and shows the most likely result.  On the other hand, LogisticRegression uses a linear function that predict the outcomes based on passed-in features.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

If the time and resource permit, I would like to try deep learning algorithms with both HyperDrive and AutoML.  As deep learning algorithms are known to discover the non-linear relations between features and lables, it will be intersting to see if deep learning algorithms can outperform traditional ML models in this project.

Another improvement is about HyperDrive configuration.  From some online research, it seems that hyperparametern sampling can have 2 stages: the first stage uses RandomParameterSampling for initial hyperparameter search, and then the second stage uses BayesianParameterSampling to refine the search for the best hyperparameter.  In addition, we can try another classification algorithm, such as RandomForester.  I'd like to see if the refined 2-stage hyperparameter tuning combined with a different algorithm can beat AutoML.

## Proof of cluster clean up

As shown in the Jupyter Notebook, the cluster was deleted in the end. 
